//
//  MainViewController.swift
//  Lawing-iOS
//
//  Created by ì¡°í˜œë¦° on 5/29/24.
//

import UIKit
import AVFoundation
import CoreLocation
import CoreML
import Vision

import RxCocoa
import RxRelay
import RxSwift
import SnapKit

enum DetectHelmet: String {
    case helmet
    case nonHelmet
    case error
}

enum MainViewState {
    case before
    case start
}

final class MainViewController: UIViewController {
    
    var disposeBag: DisposeBag = DisposeBag()
    var detectDisposeBag: DisposeBag = DisposeBag()
    
    var viewState: MainViewState = .before {
        didSet {
            if viewState == .start {
                setupStart()
            }
        }
    }
    var faceDetectResult: BehaviorRelay<[Int]> = BehaviorRelay(value: [])
    var helmetDetectResult: BehaviorRelay<[DetectHelmet]> = BehaviorRelay(value: [])
    var velocity: CGFloat = 0
    
    // MARK: - UI Property

    private let beforeStartView: BeforeStartView = BeforeStartView()
    private let drivingView: DrivingView = DrivingView()
    let label = UILabel()
    
    // MARK: - location Property
    
    let locationManager = CLLocationManager()
    
    // MARK: - camera Property

    // captureSession : ìž…ë ¥ì—ì„œ ì¶œë ¥ ìž¥ì¹˜ë¡œì˜ ë°ì´í„° íë¦„ì„ ì œì–´í•˜ëŠ”ë° ì‚¬ìš©
    // captureDevice : captureí•˜ëŠ” ë¬¼ë¦¬ì ì¸ deviceë¥¼ ì˜ë¯¸
    let captureSession = AVCaptureSession()
    var captureDevice : AVCaptureDevice? = nil
    
    // captureDeviceë¥¼ í†µí•œ ì¶œë ¥ë“¤
    // ì¶”í›„ì—,
    // captureDeviceë¥¼ í†µí•œ Inputì„ ìœ„ì—ì„œ ìƒì„±í•œ captureSessionì— ì¶”ê°€
    // captureDeviceë¥¼ í†µí•œ Outputì„ ìœ„ì—ì„œ ìƒì„±í•œ captureSessionì— ì¶”ê°€
    let photoOutput = AVCapturePhotoOutput()
    let videoOutput = AVCaptureVideoDataOutput()
    
    // Input, Outputì´ ì¶”ê°€ëœ captureSessionì˜ ê°ì²´ë¥¼ ë°›ì•„ ë¯¸ë¦¬ë³´ê¸° í™”ë©´ì„ êµ¬ì„±í•˜ëŠ” Layer
    // Layerì´ë¯€ë¡œ UIViewë¥¼ ë”°ë¡œ ë§Œë“¤ì–´, ë§Œë“  UIViewì— ì´ previewLayerë¥¼ ë¶€ì°©í•´ì¤˜ì•¼ í•¨
    var previewLayer : AVCaptureVideoPreviewLayer?
    
    // captureSessionì„ ìœ„í•œ ë°±ê·¸ë¼ìš´ë“œ í ìƒì„±
    let sessionQueue = DispatchQueue(label: "session queue")
    
    private var drawings: [CAShapeLayer] = []
    private var labels: [UILabel] = []
    
    override func viewDidLoad() {
        super.viewDidLoad()
        
        setupLocationManager()
        setupCaptureDevice()
        setupCaptureSession()
        setupStyle()
        setupBeforeView()
        setupBeforeMultiface()
    }
    
    override func viewWillAppear(_ animated: Bool) {
        super.viewWillAppear(animated)
        
        getCameraFrames()
        startSession()
    }
    
    override func viewDidDisappear(_ animated: Bool) {
        super.viewDidDisappear(animated)
        
        stopSession()
    }
}

private extension MainViewController {
    
    func setupBeforeView() {
        beforeStartView.retryHelmetView.retryButton
            .rx.tap
            .withUnretained(self)
            .subscribe(onNext: { vc, _ in                
                vc.helmetDetectResult.accept([])
                vc.setupBeforeHelmet()
            }).disposed(by: disposeBag)
        
        beforeStartView.retryMultiFaceView.retryButton
            .rx.tap
            .withUnretained(self)
            .subscribe(onNext: { vc, _ in
                vc.faceDetectResult.accept([])
                vc.setupBeforeMultiface()
            }).disposed(by: disposeBag)
        
        helmetDetectResult
            .withUnretained(self)
            .subscribe(onNext: { vc, result in
                if result.count >= 100 {
                    if vc.detectHelmet(detectResult: result) {
                        vc.label.textColor = .black
                        vc.label.backgroundColor = UIColor.green
                        vc.label.text = "helmet"
                    } else {
                        vc.label.textColor = .white
                        vc.label.backgroundColor = UIColor.systemRed
                        vc.label.text = "nonHelmet"
                    }
                    vc.label.sizeToFit()
                }
        }).disposed(by: disposeBag)
    }
    
    func setupBeforeMultiface() {
        var detectMultiFace: Bool = false

        beforeStartView.isHidden = false
        drivingView.isHidden = true
        
        beforeStartView.setupMultiFace()
        faceDetectResult
            .withUnretained(self)
            .subscribe(onNext: { vc, result in
                if result.count >= 100 {
                    detectMultiFace = vc.detectMultiface(detectResult: result)
                    vc.beforeStartView.resultDetectMultiFace(isCorrect: !detectMultiFace)
                    if !detectMultiFace {
                        vc.helmetDetectResult.accept([])
                        vc.setupBeforeHelmet()
                        vc.detectDisposeBag = DisposeBag()
                    }
                }
        }).disposed(by: detectDisposeBag)
    }
    
    func setupBeforeHelmet() {
        var detectHelmet: Bool = false

        beforeStartView.isHidden = false
        drivingView.isHidden = true
        
        beforeStartView.setupHelmet()
        
        helmetDetectResult
            .withUnretained(self)
            .subscribe(onNext: { vc, result in
                print(result.count)
                if result.count >= 100 {
                    detectHelmet = vc.detectHelmet(detectResult: result)
                    vc.beforeStartView.resultDetectHelmet(isCorrect: detectHelmet)
                    if detectHelmet {
                        vc.detectDisposeBag = DisposeBag()
                        vc.beforeStartView.correctAllState()
                        DispatchQueue.main.asyncAfter(deadline: .now() + 2.0) {
                            vc.helmetDetectResult.accept([])
                            vc.setupStart()
                        }
                    }
                }
        }).disposed(by: disposeBag)
    }
    
    func setupStart() {
        var detectMultiFace: Bool = false
        var detectHelmet: Bool = false

        beforeStartView.isHidden = true
        drivingView.isHidden = false
        
        faceDetectResult
            .withUnretained(self)
            .subscribe(onNext: { vc, result in
                if result.count >= 100 {
                    detectMultiFace = vc.detectMultiface(detectResult: result)
                    if detectMultiFace {
                        vc.drivingView.multiFaceWarningView.isHidden = false
                        vc.drivingView.helmetWarningView.isHidden = true
                        vc.drivingView.velocityWarningView.isHidden = true
                    } else {
                        let helmetResult = vc.helmetDetectResult.value
                        detectHelmet = vc.detectHelmet(detectResult: helmetResult)
                        if !detectHelmet {
                            vc.drivingView.multiFaceWarningView.isHidden = true
                            vc.drivingView.helmetWarningView.isHidden = false
                            vc.drivingView.velocityWarningView.isHidden = true
                        } else {
                            let velocity = vc.velocity
                            if velocity > 25 {
                                vc.drivingView.multiFaceWarningView.isHidden = true
                                vc.drivingView.helmetWarningView.isHidden = true
                                vc.drivingView.velocityWarningView.isHidden = false
                            } else {
                                vc.drivingView.multiFaceWarningView.isHidden = true
                                vc.drivingView.helmetWarningView.isHidden = true
                                vc.drivingView.velocityWarningView.isHidden = true
                            }
                        }
                    }
                }
        }).disposed(by: detectDisposeBag)
    }
}

// MARK: - Connect Camera

private extension MainViewController {
    // ì¹´ë©”ë¼ë¥¼ í†µí•´ VideoOutput ì„¸íŒ… í›„ Sessionì— ì—°ê²°
    // ì´ë¥¼ í†µí•´ ì‹¤ì‹œê°„ìœ¼ë¡œ Video Frameì„ ì²˜ë¦¬í•  ìˆ˜ ìžˆëŠ” í™˜ê²½ ì¤€ë¹„ë¨
    func getCameraFrames() {
        
        // 1. video frameì˜ í”½ì…€ í˜•ì‹ ì§€ì • (ê° í”½ì…€ì´ 32ë¹„íŠ¸, BGRAìˆœìœ¼ë¡œ ì €ìž¥)
        // 2. ì²˜ë¦¬ ì†ë„ê°€ ëŠë ¤ì§€ë©´ ëŠ¦ê²Œ ë„ì°©í•œ video frame íê¸°
        // 3. VideoOutputì˜ sampleBuffer ë¸ë¦¬ê²Œì´íŠ¸ ì„¸íŒ…
        //    ì—¬ê¸°ì„œ í”„ë ˆìž„ ì²˜ë¦¬ë¥¼ ìœ„í•œ íê°€ í•„ìš”í•œë°, ì´ ì—­ì‹œ ì§€ì •í•´ì¤Œ
        
        videoOutput.videoSettings = [(kCVPixelBufferPixelFormatTypeKey as NSString): NSNumber(value: kCVPixelFormatType_32BGRA)] as [String: Any]
        videoOutput.alwaysDiscardsLateVideoFrames = true
        videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "camera_frame_processing_queue"))
        
        // VideoOutputì´ Sessionì— ì¶”ê°€ë˜ì–´ìžˆì§€ ì•Šë‹¤ë©´ ì¶”ê°€
        if !captureSession.outputs.contains(videoOutput) {
            captureSession.addOutput(videoOutput)
        }
        
        // Video ë°©í–¥ì„ ì„¸ë¡œë¡œ ì„¤ì •í•˜ê¸° ìœ„í•¨
        if let connection = videoOutput.connection(with: .video), connection.isVideoOrientationSupported {
            connection.videoOrientation = .portrait
        }
    }
    
    func startSession() {
        sessionQueue.async { [weak self] in
            self?.captureSession.startRunning()
        }
    }
    
    func stopSession() {
        captureSession.stopRunning()
    }
}

private extension MainViewController {
    func clearDrawings() {
        for drawing in drawings {
            drawing.removeFromSuperlayer()
        }
        
        for label in labels {
            label.removeFromSuperview()
        }
        drawings.removeAll()
        labels.removeAll()
    }
    
    private func detectFace(image: CVPixelBuffer) {
        let faceDetectionRequest = VNDetectFaceLandmarksRequest { [weak self] vnRequest, error in
            DispatchQueue.main.async { [weak self] in
                guard let self else { return }
                if let results = vnRequest.results as? [VNFaceObservation], results.count > 0 {
                    self.handleFaceDetectionResults(observedFaces: results, pixelBuffer: image)
                    var tempResult = self.faceDetectResult.value
                    if tempResult.count >= 100 {
                        tempResult.removeFirst()
                    }
                    tempResult.append(results.count)
                    self.faceDetectResult.accept(tempResult)
                }
            }
        }
        
        let imageResultHandler = VNImageRequestHandler(cvPixelBuffer: image, orientation: .leftMirrored, options: [:])
        try? imageResultHandler.perform([faceDetectionRequest])
    }
    
    func handleFaceDetectionResults(observedFaces: [VNFaceObservation], pixelBuffer: CVPixelBuffer) {
        
        clearDrawings()
        
        guard let previewLayer = previewLayer else {
            return
        }
        for faceObservation in observedFaces {
            
            let faceBoundingBoxOnScreen = previewLayer.layerRectConverted(fromMetadataOutputRect: faceObservation.boundingBox)
            let faceBoundingBoxPath = CGPath(rect: faceBoundingBoxOnScreen, transform: nil)
            let faceBoundingBoxShape = CAShapeLayer()
            
            faceBoundingBoxShape.strokeColor = UIColor.systemBlue.cgColor
            faceBoundingBoxShape.path = faceBoundingBoxPath
            faceBoundingBoxShape.fillColor = UIColor.clear.cgColor
            view.layer.addSublayer(faceBoundingBoxShape)
            drawings.append(faceBoundingBoxShape)
            
            // ì–¼êµ´ ì¸ì‹ëœ ë¶€ë¶„ì„ CIImageë¡œ ì¶”ì¶œ
            let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
            let width = CGFloat(CVPixelBufferGetWidth(pixelBuffer))
            let height = CGFloat(CVPixelBufferGetHeight(pixelBuffer))
            
            // faceObservation.boundingBoxëŠ” ì •ê·œí™”ëœ ì¢Œí‘œ (0.0 - 1.0)ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì‹¤ì œ í¬ê¸°ë¡œ ë³€í™˜
            var faceRect = CGRect(
                x: faceObservation.boundingBox.origin.x * width,
                y: (1 - faceObservation.boundingBox.origin.y - faceObservation.boundingBox.height) * height,
                width: faceObservation.boundingBox.width * width,
                height: faceObservation.boundingBox.height * height
            )
            
            // faceRect ì¤‘ì‹¬ì  ê³„ì‚°
            let centerX = faceRect.midX
            let centerY = faceRect.midY
            
            // ìƒˆë¡œìš´ í¬ê¸° ê³„ì‚° (1.5ë°°)
            let newWidth = faceRect.width * 1.5
            let newHeight = faceRect.height * 1.5
            
            // ìƒˆë¡œìš´ faceRect ìƒì„±
            faceRect = CGRect(
                x: centerX - newWidth / 2,
                y: centerY - newHeight / 2,
                width: newWidth,
                height: newHeight
            )
            
            
            // ì–¼êµ´ ë¶€ë¶„ë§Œ ìžë¥¸ CIImage ìƒì„±
            let croppedImage = ciImage.cropped(to: faceRect)
            
            // ìžë¥¸ ì–¼êµ´ ë¶€ë¶„ìœ¼ë¡œ helmet detect
            detectHelmet(image: croppedImage) { [weak self] result in
                guard let self else { return }
                var tempResult = self.helmetDetectResult.value
                if tempResult.count >= 100 {
                    tempResult.removeFirst()
                }
                tempResult.append(result)
                self.helmetDetectResult.accept(tempResult)
                
                // UILabel ìœ„ì¹˜ ì„¤ì •
                let labelX = faceBoundingBoxOnScreen.origin.x
                let labelY = faceBoundingBoxOnScreen.origin.y - label.frame.height
                label.frame.origin = CGPoint(x: labelX, y: labelY)
                
                // UILabelì„ viewì— ì¶”ê°€
                self.view.addSubview(label)
                self.labels.append(label)
            }
        }
    }
    
    private func detectHelmet(image: CIImage, completion: @escaping (DetectHelmet) -> Void) {
        // coreML ìƒì„±
        guard let coreMLModel = try? lawingHelmet3(configuration: MLModelConfiguration()),
              let visionModel = try? VNCoreMLModel(for: coreMLModel.model) else {
            fatalError("Loading CoreML Model Failed")
        }
        
        // request ìƒì„±
        let request = VNCoreMLRequest(model: visionModel) { [weak self] request, error in
            guard error == nil else {
                fatalError("Failed Request")
            }
            
            // ì‹ë³„ìžì˜ ì´ë¦„ì„ í™•ì¸í•˜ê¸° ìœ„í•´ VNClassificationObservationë¡œ ë³€í™˜
            guard let classification = request.results as? [VNClassificationObservation] else {
                fatalError("Faild convert VNClassificationObservation")
            }
            
            // ë¨¸ì‹ ëŸ¬ë‹ì„ í†µí•œ ê²°ê³¼ê°’ í”„ë¦°íŠ¸
            //                        print("ðŸ”¥\(classification)")
            if let fitstItem = classification.first {
                if fitstItem.identifier.capitalized.lowercased() == "helmet" {
                    //                    self?.isDetectHelmet = true
                    completion(.helmet)
                } else if fitstItem.identifier.capitalized.lowercased() == "nonhelmet" {
                    //                    self?.isDetectHelmet = false
                    completion(.nonHelmet)
                } else {
                    print("ê°ì§€ ì‹¤íŒ¨")
                    completion(.error)
                }
            }
        }
        
        // ì´ë¯¸ì§€ë¥¼ ë°›ì•„ì™€ì„œ performì„ ìš”ì²­í•˜ì—¬ ë¶„ì„í•œë‹¤. (Vision í”„ë ˆìž„ì›Œí¬)
        let handler = VNImageRequestHandler(ciImage: image)
        do {
            try handler.perform([request])
        } catch {
            print(error)
            completion(.error)
        }
    }
}

private extension MainViewController {
    func setupLocationManager() {
        locationManager.delegate = self
        locationManager.requestWhenInUseAuthorization()
        locationManager.desiredAccuracy = kCLLocationAccuracyBest
        locationManager.startUpdatingLocation()
    }
    
    // MARK: - captureDevice Setting
    
    func setupCaptureDevice() {
        // ìš°ì„ ì ìœ¼ë¡œ ì „ë©´ì˜ dualCameraë¡œ CaptureDevice ì„¸íŒ…,
        // ì—†ë‹¤ë©´ wideAngleCameraë¡œ CaptureDevice ì„¸íŒ…
        if let device = AVCaptureDevice.default(.builtInDualCamera,
                                                for: .video,
                                                position: .front) {
            captureDevice = device
        } else if let device = AVCaptureDevice.default(.builtInWideAngleCamera,
                                                       for: .video,
                                                       position: .front) {
            captureDevice = device
        } else {
            fatalError("Missing expected back camera device.")
        }
    }
    
    // MARK: - captureSession Setting
    
    func setupCaptureSession() {
        guard let captureDevice else { return }
        
        // captureSessionì˜ presetì„ photoë¡œ ì„¸íŒ…
        captureSession.sessionPreset = AVCaptureSession.Preset.photo
        do {
            // captureDeviceì˜ Input ì„¸íŒ…
            try captureSession.addInput(AVCaptureDeviceInput(device: captureDevice))
            
            // captureDeviceì˜ Output ì„¸íŒ…
            if captureSession.canAddOutput(photoOutput) {
                captureSession.addOutput(photoOutput)
            }
            
        }
        catch {
            print("error: \(error.localizedDescription)")
        }
        
        // ìœ„ì—ì„œ ì„¸íŒ…í•œ CaptureSessionì„ PreviewLayerì— ë„£ì–´ì¤Œ
        previewLayer = AVCaptureVideoPreviewLayer(session: captureSession)
        guard let previewLayer else { return }
        previewLayer.frame = UIScreen.main.bounds
        previewLayer.videoGravity = .resizeAspectFill
        view.layer.addSublayer(previewLayer)
        setupLayout()
        
        // captureSessionì˜ ë³€ê²½ì‚¬í•­ì„ ì ìš©(ì»¤ë°‹)í•˜ëŠ” ì½”ë“œ
        captureSession.commitConfiguration()
    }
}

private extension MainViewController {
    func setupStyle() {
        
    }
    
    func setupLayout() {
        view.addSubviews(beforeStartView,
                        drivingView)
        
        beforeStartView.snp.makeConstraints {
            $0.edges.equalToSuperview()
        }
        
        drivingView.snp.makeConstraints {
            $0.edges.equalToSuperview()
        }
    }
}

extension MainViewController: CLLocationManagerDelegate {
    func locationManager(_ manager: CLLocationManager, didUpdateLocations locations: [CLLocation]) {
        guard let currentLocation = locations.last else { return }
        let currentSpeed = currentLocation.speed
        
        print("Current Speed: \(currentSpeed * 3.6) m/s")
        
        drivingView.velocityView.bindView(velocity: currentSpeed * 3.6)
        velocity = currentSpeed * 3.6
    }
    
    func locationManager(_ manager: CLLocationManager, didFailWithError error: Error) {
        print("Location update failed: \(error.localizedDescription)")
    }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate

extension MainViewController: AVCaptureVideoDataOutputSampleBufferDelegate {
    // ì‹¤ì‹œê°„ìœ¼ë¡œ video frameì„ ë°›ì•„ì˜¬ ë•Œë§ˆë‹¤ ì‹¤í–‰ë¨
    func captureOutput(_ output: AVCaptureOutput,
                       didOutput sampleBuffer: CMSampleBuffer,
                       from connection: AVCaptureConnection) {
        guard let frame = CMSampleBufferGetImageBuffer(sampleBuffer) else {
            return
        }
        detectFace(image: frame)
    }
}

extension MainViewController {
    /// trueê°€ ë‹¤ì¤‘ ê°ì§€
    func detectMultiface(detectResult: [Int]) -> Bool {
        let count = detectResult.count
        if CGFloat(detectResult.filter { $0 > 1 }.count) / CGFloat(count) > 0.1 {
            return true
        } else {
            return false
        }
    }
    
    /// trueê°€ í—¬ë©§ì“´ ê²ƒ
    func detectHelmet(detectResult: [DetectHelmet]) -> Bool {
        if detectResult.filter({ $0 == .helmet }).count >=
            detectResult.filter({ $0 != .helmet }).count {
            return true
        } else {
            return false
        }
    }
}
